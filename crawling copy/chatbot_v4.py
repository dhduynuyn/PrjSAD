import os
import time
import pickle
import faiss
import numpy as np
from dotenv import load_dotenv
from typing import Any, Dict, List, Optional, Mapping

from sentence_transformers import SentenceTransformer
from gpt4all import GPT4All
from langchain_core.language_models.llms import LLM
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import Document

# --- C·∫•u h√¨nh ---
load_dotenv()
EMBED_MODEL_NAME = "keepitreal/vietnamese-sbert"
FAISS_INDEX_PATH = "story_faiss.index"
METADATA_PATH = "stories_embeddings.pkl"
LOCAL_MODEL_NAME = "Mistral-7B-Instruct-v0.1.Q4_0.gguf"
MODEL_DOWNLOAD = "Llama-3.2-1B-Instruct-Q4_0.gguf"
BASE_DIR = os.getcwd()
LOCAL_MODEL_DIR = os.path.join(BASE_DIR, "local_model")  # Tr·ªè v√†o th∆∞ m·ª•c local_model

# --- T·∫£i embedding model ---
embedding_model = SentenceTransformer(EMBED_MODEL_NAME)

# --- T·∫£i FAISS v√† metadata ---
faiss_index = faiss.read_index(FAISS_INDEX_PATH)
with open(METADATA_PATH, "rb") as f:
    story_metadata = pickle.load(f)

# --- H√†m t√¨m ki·∫øm ---
def find_similar_stories(user_query, top_k=3):
    query_embedding = embedding_model.encode([user_query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_embedding, top_k)
    results = []

    for i in indices[0]:
        if i < len(story_metadata):
            story = story_metadata[i]
            results.append(Document(page_content=f"üìò {story['title']}:\n{story['summary']}", metadata=story))
    return results

# --- Prompt m·∫´u (gi·ªëng Llama/Mistral style) ---
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "<|system|>B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ truy·ªán Vi·ªát Nam, gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi ho·∫∑c g·ª£i √Ω truy·ªán.\n"
        "<|user|>\n"
        "D·ª±a v√†o truy·ªán sau:\n{context}\n\nC√¢u h·ªèi: {question}\n"
        "<|assistant|>"
    )
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# --- Kh·ªüi t·∫°o LLM local ---
class CustomGPT4All(LLM):
    model: Any
    model_path: str

    @property
    def _llm_type(self) -> str:
        return "custom_gpt4all"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.model.generate(
            prompt,
            max_tokens=512,
            temp=0.7,
            top_k=40,
            top_p=0.9,
            repeat_penalty=1.2,
        )
        return response

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model_path": self.model_path}

print("üöÄ ƒêang kh·ªüi t·∫°o m√¥ h√¨nh local...")
gpt4all_model = GPT4All(model_name=LOCAL_MODEL_NAME, model_path=LOCAL_MODEL_DIR, allow_download=False)
llm = CustomGPT4All(model=gpt4all_model, model_path=LOCAL_MODEL_DIR)
print("‚úÖ M√¥ h√¨nh ƒë√£ s·∫µn s√†ng.")

# --- Chain h·ªôi tho·∫°i ---
from langchain_core.runnables import RunnableLambda

rag_chain = (
    {"context": find_similar_stories, "question": RunnablePassthrough()}
    | RunnableLambda(lambda x: {"context": format_docs(x["context"]), "question": x["question"]})
    | prompt_template
    | llm
)

# --- H√†m h·ªôi tho·∫°i ch√≠nh ---
def get_story_response(query, chat_history):
    start = time.time()
    result = rag_chain.invoke(query)
    elapsed = time.time() - start

    chat_history.append((query, result))
    return result, elapsed

# --- Giao di·ªán d√≤ng l·ªánh ƒë∆°n gi·∫£n (t√πy ch·ªçn) ---
if __name__ == "__main__":
    chat_history = []
    print("üß† Chatbot truy·ªán Vi·ªát Nam - D√πng LLM n·ªôi b·ªô\n---")
    while True:
        query = input("üë§ B·∫°n: ")
        if query.lower() in ["exit", "quit", "tho√°t"]:
            break
        response, _ = get_story_response(query, chat_history)
        print("ü§ñ:", response)
